{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RepeatBuyers_din.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "x1YpH5ygv62I"
      ],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qc66bhQ3w4gB",
        "colab_type": "text"
      },
      "source": [
        "# 天猫用户复购预测"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yrVLObec1Tr2",
        "colab_type": "text"
      },
      "source": [
        "* Content：天猫用户复购预测,使用Attention机制的DNN模型完成预测;\n",
        "分享交流经验;Score > 0.66\n",
        "* Author:  HuiHui\n",
        "* Date:    2020-03-28\n",
        "* Reference:\n",
        "* 数据集：该数据集包含“双11”前6个月和“双11”当天匿名用户的购物日志，以及显示他们是否为重复购买者的标签信息。\n",
        "    * label: 1'表示'user_id'是'merchant_id'的重复买家，而'0'则相反。'-1'表示'user_id'不是给定商家的新客户\n",
        "    * activity_log: {用户id，商家id}之间的一组交互记录，其中每个记录都是一个动作，表示为“项目id:category id:brand id:time\\u stamp:action\\u type”#'用于分隔两个相邻元素。记录不按任何特定顺序排序"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "iLPjG3VFu92f",
        "outputId": "b6fb2bdb-bf11-4264-ebcf-642580d6dbf8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive') #挂载网盘"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "buQv_6iYvKzc",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.chdir(\"/content/gdrive/My Drive/RS/Repeat Buyers Prediction\") #改变当前工作目录到指定的路径"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "06hB_RrTSExt",
        "colab_type": "code",
        "outputId": "577508fc-e059-48c0-89cd-dfe64da314a4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "!pip install tqdm\n",
        "%tensorflow_version 1.x\n",
        "!pip install -q deepctr[gpu]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (4.38.0)\n",
            "TensorFlow 1.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c6o_ntMWXYGS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!pip uninstall -y tensorflow\n",
        "#!pip install tensorflow-gpu==1.14.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xcIfzzTkpq4M",
        "colab_type": "code",
        "outputId": "525e2ba1-a70e-4866-e547-6c00b6a43593",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        }
      },
      "source": [
        "#环境\n",
        "import sys\n",
        "print(sys.version)\n",
        "\n",
        "import deepctr\n",
        "print(deepctr.__version__)\n",
        "\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "\n",
        "if tf.test.gpu_device_name():\n",
        "  print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))\n",
        "else:\n",
        "  print(\"Please install GPU version of TF\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3.6.9 (default, Nov  7 2019, 10:44:02) \n",
            "[GCC 8.3.0]\n",
            "0.7.4\n",
            "1.15.2\n",
            "Default GPU Device: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wgluqI-xSiKC",
        "colab_type": "code",
        "outputId": "85f6033d-caf8-4b0d-a189-5dbe2202769d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        }
      },
      "source": [
        "import gc\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# time1=time.time()\n",
        "# time2=time.time()\n",
        "# print(time2-time1)\n",
        "\n",
        "# 用户行为，使用format1进行加载\n",
        "# 加载全量样本\n",
        "user_log = pd.read_csv('./Repeat Buyers Prediction DataSet/data_format1/user_log_format1.csv', dtype={'time_stamp':'str'})\n",
        "user_info = pd.read_csv('./Repeat Buyers Prediction DataSet/data_format1/user_info_format1.csv')\n",
        "train_data1 = pd.read_csv('./Repeat Buyers Prediction DataSet/data_format1/train_format1.csv')\n",
        "submission = pd.read_csv('./Repeat Buyers Prediction DataSet/data_format1/test_format1.csv')\n",
        "\n",
        "# # 加载小样本\n",
        "# user_log = pd.read_csv('./Repeat Buyers Prediction DataSet/data_format1_small/sample_user_log.csv', dtype={'time_stamp':'str'})\n",
        "# user_info = pd.read_csv('./Repeat Buyers Prediction DataSet/data_format1_small/sample_user_info.csv')\n",
        "# train_data1 = pd.read_csv('./Repeat Buyers Prediction DataSet/data_format1_small/train.csv')\n",
        "# submission = pd.read_csv('./Repeat Buyers Prediction DataSet/data_format1_small/test.csv')\n",
        "\n",
        "train_data = pd.read_csv('./Repeat Buyers Prediction DataSet/data_format2/train_format2.csv')\n",
        "\n",
        "###### 数据处理 #######\n",
        "train_data1['origin'] = 'train'\n",
        "submission['origin'] = 'test'\n",
        "matrix = pd.concat([train_data1, submission], ignore_index=True, sort=False)\n",
        "#print(matrix.head())\n",
        "\n",
        "# 使用merchant_id（原列名seller_id）\n",
        "user_log.rename(columns={'seller_id':'merchant_id'}, inplace=True)\n",
        "# 格式化\n",
        "user_log['user_id'] = user_log['user_id'].astype('int32')\n",
        "user_log['merchant_id'] = user_log['merchant_id'].astype('int32')\n",
        "user_log['item_id'] = user_log['item_id'].astype('int32')\n",
        "user_log['cat_id'] = user_log['cat_id'].astype('int32')\n",
        "user_log['brand_id'].fillna(0, inplace=True)\n",
        "user_log['brand_id'] = user_log['brand_id'].astype('int32')\n",
        "user_log['time_stamp'] = pd.to_datetime(user_log['time_stamp'], format='%H%M')\n",
        "\n",
        "# 对离散特征做LabelEncoder（0～n-1）\n",
        "lbe_merchant_id=LabelEncoder()\n",
        "lbe_merchant_id.fit(np.r_[0,user_log['merchant_id'].values])\n",
        "user_log['merchant_id']=lbe_merchant_id.transform(user_log['merchant_id'])\n",
        "matrix['merchant_id']=lbe_merchant_id.transform(matrix['merchant_id'])\n",
        "\n",
        "lbe_user_id=LabelEncoder()\n",
        "user_log['user_id']=lbe_user_id.fit_transform(user_log['user_id'])\n",
        "user_info['user_id']=lbe_user_id.transform(user_info['user_id'])\n",
        "matrix['user_id']=lbe_user_id.transform(matrix['user_id'])\n",
        "\n",
        "lbe_item_id=LabelEncoder()\n",
        "user_log['item_id']=lbe_item_id.fit_transform(user_log['item_id'])\n",
        "lbe_cat_id=LabelEncoder()\n",
        "user_log['cat_id']=lbe_cat_id.fit_transform(user_log['cat_id'])\n",
        "lbe_brand_id=LabelEncoder()\n",
        "user_log['brand_id']=lbe_brand_id.fit_transform(user_log['brand_id'])\n",
        "\n",
        "user_log['merchant_id'].max(),user_log['user_id'].max() #统计不同商家个数：4994+1，不同买家的个数：19111+1(小样本时)\n",
        "print(user_log['merchant_id'].max())\n",
        "print(user_log['user_id'].max())\n",
        "matrix = matrix.merge(user_info, on='user_id', how='left')\n",
        "\n",
        "# 1 for <18; 2 for [18,24]; 3 for [25,29]; 4 for [30,34]; 5 for [35,39]; 6 for [40,49]; 7 and 8 for >= 50; 0 and NULL for unknown\n",
        "matrix['age_range'].fillna(0, inplace=True)\n",
        "# 0:female, 1:male, 2:unknown\n",
        "matrix['gender'].fillna(2, inplace=True)\n",
        "matrix['age_range'] = matrix['age_range'].astype('int8')\n",
        "matrix['gender'] = matrix['gender'].astype('int8')\n",
        "matrix['label'] = matrix['label'].astype('str')\n",
        "matrix['user_id'] = matrix['user_id'].astype('int32')\n",
        "matrix['merchant_id'] = matrix['merchant_id'].astype('int32')\n",
        "del user_info, train_data1\n",
        "gc.collect()\n",
        "\n",
        "# User特征处理\n",
        "groups = user_log.groupby(['user_id'])\n",
        "# 用户交互行为数量 u1\n",
        "temp = groups.size().reset_index().rename(columns={0:'u1'})\n",
        "matrix = matrix.merge(temp, on='user_id', how='left')\n",
        "# 使用agg 基于列的聚合操作，统计唯一值的个数 item_id, cat_id, merchant_id, brand_id\n",
        "#temp = groups['item_id', 'cat_id', 'merchant_id', 'brand_id'].nunique().reset_index().rename(columns={'item_id':'u2', 'cat_id':'u3', 'merchant_id':'u4', 'brand_id':'u5'})\n",
        "temp = groups['item_id'].agg([('u2', 'nunique')]).reset_index()\n",
        "matrix = matrix.merge(temp, on='user_id', how='left')\n",
        "temp = groups['cat_id'].agg([('u3', 'nunique')]).reset_index()\n",
        "matrix = matrix.merge(temp, on='user_id', how='left')\n",
        "temp = groups['merchant_id'].agg([('u4', 'nunique')]).reset_index()\n",
        "matrix = matrix.merge(temp, on='user_id', how='left')\n",
        "temp = groups['brand_id'].agg([('u5', 'nunique')]).reset_index()\n",
        "matrix = matrix.merge(temp, on='user_id', how='left')\n",
        "\n",
        "# 时间间隔特征 u6 按照小时\n",
        "temp = groups['time_stamp'].agg([('F_time', 'min'), ('L_time', 'max')]).reset_index()\n",
        "temp['u6'] = (temp['L_time'] - temp['F_time']).dt.seconds/3600 #用户一开始在淘宝买东西和最近一次在淘宝买东西的间隔时间\n",
        "matrix = matrix.merge(temp[['user_id', 'u6']], on='user_id', how='left')\n",
        "# 统计action_type为0，1，2，3的个数（原始操作，没有补0）\n",
        "temp = groups['action_type'].value_counts().unstack().reset_index().rename(columns={0:'u7', 1:'u8', 2:'u9', 3:'u10'})\n",
        "matrix = matrix.merge(temp, on='user_id', how='left')\n",
        "#print(matrix)\n",
        "\n",
        "# 商家特征处理\n",
        "groups = user_log.groupby(['merchant_id'])\n",
        "# 商家被交互行为数量 m1\n",
        "temp = groups.size().reset_index().rename(columns={0:'m1'})\n",
        "matrix = matrix.merge(temp, on='merchant_id', how='left')\n",
        "# 统计商家被交互的user_id, item_id, cat_id, brand_id 唯一值\n",
        "temp = groups['user_id', 'item_id', 'cat_id', 'brand_id'].nunique().reset_index().rename(columns={'user_id':'m2', 'item_id':'m3', 'cat_id':'m4', 'brand_id':'m5'})\n",
        "matrix = matrix.merge(temp, on='merchant_id', how='left')\n",
        "# 统计商家被交互的action_type 唯一值\n",
        "temp = groups['action_type'].value_counts().unstack().reset_index().rename(columns={0:'m6', 1:'m7', 2:'m8', 3:'m9'})\n",
        "matrix = matrix.merge(temp, on='merchant_id', how='left')\n",
        "# 按照merchant_id 统计随机负采样的个数\n",
        "temp = train_data[train_data['label']==-1].groupby(['merchant_id']).size().reset_index().rename(columns={0:'m10'})\n",
        "matrix = matrix.merge(temp, on='merchant_id', how='left')\n",
        "\n",
        "# 按照user_id, merchant_id分组\n",
        "groups = user_log.groupby(['user_id', 'merchant_id'])\n",
        "temp = groups.size().reset_index().rename(columns={0:'um1'}) #统计行为个数\n",
        "matrix = matrix.merge(temp, on=['user_id', 'merchant_id'], how='left')\n",
        "temp = groups['item_id', 'cat_id', 'brand_id'].nunique().reset_index().rename(columns={'item_id':'um2', 'cat_id':'um3', 'brand_id':'um4'}) #统计item_id, cat_id, brand_id唯一个数\n",
        "matrix = matrix.merge(temp, on=['user_id', 'merchant_id'], how='left')\n",
        "temp = groups['action_type'].value_counts().unstack().reset_index().rename(columns={0:'um5', 1:'um6', 2:'um7', 3:'um8'})#统计不同action_type唯一个数\n",
        "matrix = matrix.merge(temp, on=['user_id', 'merchant_id'], how='left')\n",
        "temp = groups['time_stamp'].agg([('first', 'min'), ('last', 'max')]).reset_index()\n",
        "temp['um9'] = (temp['last'] - temp['first']).dt.seconds/3600\n",
        "temp.drop(['first', 'last'], axis=1, inplace=True)\n",
        "matrix = matrix.merge(temp, on=['user_id', 'merchant_id'], how='left') #统计时间间隔\n",
        "\n",
        "#用户购买点击比\n",
        "matrix['r1'] = matrix['u9']/matrix['u7'] \n",
        "#商家购买点击比\n",
        "matrix['r2'] = matrix['m8']/matrix['m6'] \n",
        "#不同用户不同商家购买点击比\n",
        "matrix['r3'] = matrix['um7']/matrix['um5']\n",
        "matrix.fillna(0, inplace=True)\n",
        "# # 修改age_range字段名称为 age_0, age_1, age_2... age_8\n",
        "temp = pd.get_dummies(matrix['age_range'], prefix='age')\n",
        "matrix = pd.concat([matrix, temp], axis=1)\n",
        "temp = pd.get_dummies(matrix['gender'], prefix='g')\n",
        "matrix = pd.concat([matrix, temp], axis=1)\n",
        "matrix.drop(['age_range', 'gender'], axis=1, inplace=True)\n",
        "\n",
        "lbe_action_type={0:1,1:2,2:3,3:4}\n",
        "user_log['action_type']=user_log['action_type'].map(lbe_action_type) #action_type映射成1,2,3,4；方便后面用0补缺固定长度\n",
        "\n",
        "# 用户行为sequence\n",
        "# 把user_log里同user的这些数据合并成一个list\n",
        "temp=pd.DataFrame(user_log.groupby('user_id')['merchant_id','action_type'].agg(lambda x:list(x)))\n",
        "# 列名称改成hist_merchant_id 和 hist_action_type \n",
        "temp.columns=['hist_merchant_id','hist_action_type']\n",
        "matrix = matrix.merge(temp, on=['user_id'], how='left')\n",
        "\n",
        "# 截取，补缺到定长M个\n",
        "M=500\n",
        "for feature in ['hist_merchant_id','hist_action_type']:\n",
        "    matrix[feature]=matrix[feature].map(lambda x:np.array(x+[0]*(M-len(x)))[:M])\n",
        "#print(matrix.info())\n",
        "\n",
        "# 分割训练数据和测试数据\n",
        "train_data = matrix[matrix['origin'] == 'train'].drop(['origin'], axis=1)\n",
        "test_data = matrix[matrix['origin'] == 'test'].drop(['label', 'origin'], axis=1)\n",
        "train_X, train_y = train_data.drop(['label'], axis=1), train_data['label']\n",
        "\n",
        "#保存特征文件，调试不同算法模型\n",
        "train_X.to_csv('./train_X.csv', index=False)\n",
        "train_y.to_csv('./train_y.csv', index=False)\n",
        "test_data.to_csv('./test_data.csv', index=False)\n",
        "print(\"ok\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4995\n",
            "424169\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:108: FutureWarning: Indexing with multiple keys (implicitly converted to a tuple of keys) will be deprecated, use a list instead.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:121: FutureWarning: Indexing with multiple keys (implicitly converted to a tuple of keys) will be deprecated, use a list instead.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:149: FutureWarning: Indexing with multiple keys (implicitly converted to a tuple of keys) will be deprecated, use a list instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "ok\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zDWFbcYOALzO",
        "colab_type": "code",
        "outputId": "62f70c90-f909-4315-c678-db74bce81ad2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 757
        }
      },
      "source": [
        "##### 使用DIN模型 ######\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import log_loss\n",
        "from deepctr.inputs import SparseFeat,VarLenSparseFeat,DenseFeat,get_feature_names\n",
        "from deepctr.models import DIN, DIEN, DSIN\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "M=500\n",
        "#读取submission,存储结果\n",
        "import gc\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "submission = pd.read_csv('./Repeat Buyers Prediction DataSet/data_format1/test_format1.csv') # 加载全量样本时\n",
        "# submission = pd.read_csv('./Repeat Buyers Prediction DataSet/data_format1_small/test.csv') # 加载小样本时\n",
        "submission['origin'] = 'test'\n",
        "print(submission.head())\n",
        "\n",
        "#读取保存好的特征文件，注意这里重新读取会将'hist_merchant_id','hist_action_type'读取成str,而不是list，因此需要将train_X和test_data中的hist_merchant_id，hist_action_type转回list\n",
        "train_X=pd.read_csv('./train_X.csv')\n",
        "train_y=pd.read_csv('./train_y.csv')\n",
        "test_data=pd.read_csv('./test_data.csv')\n",
        "#将train_X和test_data中的hist_merchant_id，hist_action_type转回list\n",
        "def strlist_to_list(s):\n",
        "  s=re.sub('[\\[\\]]','',s)#删除中括号\n",
        "  s=s.replace(\"\\n\", \"\")#删除换行符号\n",
        "  s=s.split() # 以空格（一个或多个）分隔\n",
        "  s=[float(num) for num in s]\n",
        "  return s\n",
        "train_X['hist_merchant_id']=train_X['hist_merchant_id'].map(strlist_to_list)\n",
        "train_X['hist_action_type']=train_X['hist_action_type'].map(strlist_to_list)\n",
        "test_data['hist_merchant_id']=test_data['hist_merchant_id'].map(strlist_to_list)\n",
        "test_data['hist_action_type']=test_data['hist_action_type'].map(strlist_to_list)\n",
        "#print(type(train_X['hist_merchant_id'].values))\n",
        "print(\"ok\")\n",
        "\n",
        "train_X['action_type']=3 #因为这里用户在商家是购买过商品的，所以添加了一列action_type，且设为3\n",
        "\n",
        "#格式转换（将之前的特征名封装成了一个类）\n",
        "feature_columns = []\n",
        "for column in train_X.columns:\n",
        "  if column != 'hist_merchant_id' and column != 'hist_action_type':\n",
        "    #print(column)\n",
        "    num = train_X[column].nunique()\n",
        "    if num > 10000:\n",
        "        dim = 10\n",
        "    else:\n",
        "        if num > 1000:\n",
        "            dim = 8\n",
        "        else:\n",
        "            dim = 4\n",
        "    #print(num)\n",
        "    if column  == 'user_id':\n",
        "        feature_columns += [SparseFeat(column, 424169+1, embedding_dim=dim)]\n",
        "    elif column  == 'merchant_id':\n",
        "        feature_columns += [SparseFeat(column, 4995+1, embedding_dim=dim)]\n",
        "    elif column  == 'action_type':\n",
        "        feature_columns += [SparseFeat(column, 4+1, embedding_dim=dim)]\n",
        "    else:\n",
        "        feature_columns += [DenseFeat(column, 1)]\n",
        "\n",
        "#print(train_X['hist_merchant_id'].shape)\n",
        "#M = len(train_X['hist_merchant_id'])\n",
        "print('M=', M)\n",
        "\n",
        "# maxlen为历史信息的长度，vocabulary_size为onehot的长度；VarLenSparseFeat为序列类型特征\n",
        "# 注意正确使用VarLenSparseFeat，不要加参数：weight_name='hist_merchant_id'，weight_name='hist_action_type'\n",
        "feature_columns += [VarLenSparseFeat(SparseFeat('hist_merchant_id', vocabulary_size=424169+1, embedding_dim=8), maxlen=M),\n",
        "                   VarLenSparseFeat(SparseFeat('hist_action_type', vocabulary_size=4+1, embedding_dim=4),maxlen=M)]\n",
        "hist_features=['merchant_id','action_type']\n",
        "print(feature_columns)\n",
        "\n",
        "# 使用DIN模型\n",
        "# 这里要求inputs shapes删除维度（/通道数）以后形状相同;通道数位置为-1，即倒数第一个数\n",
        "model=DIN(feature_columns, hist_features)\n",
        "# 使用Adam优化器，二分类的交叉熵\n",
        "model.compile('adam', 'binary_crossentropy', metrics=['binary_crossentropy'])\n",
        "# 组装train_model_input，得到feature names，将train_X转换为字典格式\n",
        "feature_names=list(train_X.columns)\n",
        "train_model_input ={name:train_X[name].values for name in feature_names}\n",
        "\n",
        "# histroy输入必须是二维数组\n",
        "#进度条\n",
        "from tqdm import tqdm\n",
        "for fea in ['hist_merchant_id','hist_action_type']:\n",
        "    l = []\n",
        "    for i in tqdm(train_model_input[fea]):\n",
        "        l.append(i)\n",
        "    train_model_input[fea]=np.array(l) #转换成二维数组\n",
        "\n",
        "history = model.fit(train_model_input, train_y, verbose=True, epochs=10, validation_split=0.2,batch_size=512)\n",
        "\n",
        "# 转换test__model_input\n",
        "test_data['action_type']=3\n",
        "test_model_input = {name:test_data[name].values for name in feature_names}#字典\n",
        "from tqdm import tqdm\n",
        "for fea in ['hist_merchant_id','hist_action_type']:\n",
        "    l = []\n",
        "    for i in tqdm(test_model_input[fea]):\n",
        "        l.append(i)\n",
        "    test_model_input[fea]=np.array(l)#二维数组\n",
        "\n",
        "# 得到预测结果\n",
        "prob = model.predict(test_model_input)\n",
        "submission['prob'] = prob\n",
        "print(submission.head())\n",
        "submission.drop(['origin'], axis=1, inplace=True)\n",
        "submission.to_csv('./prediction.csv', index=False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   user_id  merchant_id  prob origin\n",
            "0   163968         4605   NaN   test\n",
            "1   360576         1581   NaN   test\n",
            "2    98688         1964   NaN   test\n",
            "3    98688         3645   NaN   test\n",
            "4   295296         3361   NaN   test\n",
            "ok\n",
            "M= 500\n",
            "[SparseFeat(name='user_id', vocabulary_size=424170, embedding_dim=10, use_hash=False, dtype='int32', embedding_name='user_id', group_name='default_group'), SparseFeat(name='merchant_id', vocabulary_size=4996, embedding_dim=8, use_hash=False, dtype='int32', embedding_name='merchant_id', group_name='default_group'), DenseFeat(name='prob', dimension=1, dtype='float32'), DenseFeat(name='u1', dimension=1, dtype='float32'), DenseFeat(name='u2', dimension=1, dtype='float32'), DenseFeat(name='u3', dimension=1, dtype='float32'), DenseFeat(name='u4', dimension=1, dtype='float32'), DenseFeat(name='u5', dimension=1, dtype='float32'), DenseFeat(name='u6', dimension=1, dtype='float32'), DenseFeat(name='u7', dimension=1, dtype='float32'), DenseFeat(name='u8', dimension=1, dtype='float32'), DenseFeat(name='u9', dimension=1, dtype='float32'), DenseFeat(name='u10', dimension=1, dtype='float32'), DenseFeat(name='m1', dimension=1, dtype='float32'), DenseFeat(name='m2', dimension=1, dtype='float32'), DenseFeat(name='m3', dimension=1, dtype='float32'), DenseFeat(name='m4', dimension=1, dtype='float32'), DenseFeat(name='m5', dimension=1, dtype='float32'), DenseFeat(name='m6', dimension=1, dtype='float32'), DenseFeat(name='m7', dimension=1, dtype='float32'), DenseFeat(name='m8', dimension=1, dtype='float32'), DenseFeat(name='m9', dimension=1, dtype='float32'), DenseFeat(name='m10', dimension=1, dtype='float32'), DenseFeat(name='um1', dimension=1, dtype='float32'), DenseFeat(name='um2', dimension=1, dtype='float32'), DenseFeat(name='um3', dimension=1, dtype='float32'), DenseFeat(name='um4', dimension=1, dtype='float32'), DenseFeat(name='um5', dimension=1, dtype='float32'), DenseFeat(name='um6', dimension=1, dtype='float32'), DenseFeat(name='um7', dimension=1, dtype='float32'), DenseFeat(name='um8', dimension=1, dtype='float32'), DenseFeat(name='um9', dimension=1, dtype='float32'), DenseFeat(name='r1', dimension=1, dtype='float32'), DenseFeat(name='r2', dimension=1, dtype='float32'), DenseFeat(name='r3', dimension=1, dtype='float32'), DenseFeat(name='age_0', dimension=1, dtype='float32'), DenseFeat(name='age_1', dimension=1, dtype='float32'), DenseFeat(name='age_2', dimension=1, dtype='float32'), DenseFeat(name='age_3', dimension=1, dtype='float32'), DenseFeat(name='age_4', dimension=1, dtype='float32'), DenseFeat(name='age_5', dimension=1, dtype='float32'), DenseFeat(name='age_6', dimension=1, dtype='float32'), DenseFeat(name='age_7', dimension=1, dtype='float32'), DenseFeat(name='age_8', dimension=1, dtype='float32'), DenseFeat(name='g_0', dimension=1, dtype='float32'), DenseFeat(name='g_1', dimension=1, dtype='float32'), DenseFeat(name='g_2', dimension=1, dtype='float32'), SparseFeat(name='action_type', vocabulary_size=5, embedding_dim=4, use_hash=False, dtype='int32', embedding_name='action_type', group_name='default_group'), VarLenSparseFeat(sparsefeat=SparseFeat(name='hist_merchant_id', vocabulary_size=424170, embedding_dim=8, use_hash=False, dtype='int32', embedding_name='hist_merchant_id', group_name='default_group'), maxlen=500, combiner='mean', length_name=None, weight_name=None, weight_norm=True), VarLenSparseFeat(sparsefeat=SparseFeat(name='hist_action_type', vocabulary_size=5, embedding_dim=4, use_hash=False, dtype='int32', embedding_name='hist_action_type', group_name='default_group'), maxlen=500, combiner='mean', length_name=None, weight_name=None, weight_norm=True)]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 260864/260864 [00:00<00:00, 2331063.48it/s]\n",
            "100%|██████████| 260864/260864 [00:00<00:00, 2328345.11it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 208691 samples, validate on 52173 samples\n",
            "Epoch 1/10\n",
            "208691/208691 [==============================] - 23s 108us/sample - loss: 0.9433 - binary_crossentropy: 0.9433 - val_loss: 0.9431 - val_binary_crossentropy: 0.9431\n",
            "Epoch 2/10\n",
            "208691/208691 [==============================] - 22s 104us/sample - loss: 0.9433 - binary_crossentropy: 0.9433 - val_loss: 0.9431 - val_binary_crossentropy: 0.9431\n",
            "Epoch 3/10\n",
            "208691/208691 [==============================] - 22s 106us/sample - loss: 0.9433 - binary_crossentropy: 0.9433 - val_loss: 0.9431 - val_binary_crossentropy: 0.9431\n",
            "Epoch 4/10\n",
            "208691/208691 [==============================] - 22s 105us/sample - loss: 0.9433 - binary_crossentropy: 0.9433 - val_loss: 0.9431 - val_binary_crossentropy: 0.9431\n",
            "Epoch 5/10\n",
            "208691/208691 [==============================] - 22s 104us/sample - loss: 0.9433 - binary_crossentropy: 0.9433 - val_loss: 0.9431 - val_binary_crossentropy: 0.9431\n",
            "Epoch 6/10\n",
            "208691/208691 [==============================] - 22s 104us/sample - loss: 0.9433 - binary_crossentropy: 0.9433 - val_loss: 0.9431 - val_binary_crossentropy: 0.9431\n",
            "Epoch 7/10\n",
            "208691/208691 [==============================] - 22s 104us/sample - loss: 0.9433 - binary_crossentropy: 0.9433 - val_loss: 0.9431 - val_binary_crossentropy: 0.9431\n",
            "Epoch 8/10\n",
            "208691/208691 [==============================] - 22s 104us/sample - loss: 0.9433 - binary_crossentropy: 0.9433 - val_loss: 0.9431 - val_binary_crossentropy: 0.9431\n",
            "Epoch 9/10\n",
            "208691/208691 [==============================] - 22s 103us/sample - loss: 0.9433 - binary_crossentropy: 0.9433 - val_loss: 0.9431 - val_binary_crossentropy: 0.9431\n",
            "Epoch 10/10\n",
            "208691/208691 [==============================] - 22s 103us/sample - loss: 0.9433 - binary_crossentropy: 0.9433 - val_loss: 0.9431 - val_binary_crossentropy: 0.9431\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 261477/261477 [00:00<00:00, 2427937.70it/s]\n",
            "100%|██████████| 261477/261477 [00:00<00:00, 2496333.11it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "   user_id  merchant_id  prob origin\n",
            "0   163968         4605   0.0   test\n",
            "1   360576         1581   0.0   test\n",
            "2    98688         1964   0.0   test\n",
            "3    98688         3645   0.0   test\n",
            "4   295296         3361   0.0   test\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9XT-AcfGtDBl",
        "colab_type": "code",
        "outputId": "f0b84fb0-8fc6-45ee-fd77-d8a3bce83ed7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 179
        }
      },
      "source": [
        "print(submission.describe())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "             user_id    merchant_id      prob\n",
            "count  261477.000000  261477.000000  261477.0\n",
            "mean   212121.259128    2539.620077       0.0\n",
            "std    122480.366678    1451.697856       0.0\n",
            "min         2.000000       2.000000       0.0\n",
            "25%    106317.000000    1340.000000       0.0\n",
            "50%    212289.000000    2482.000000       0.0\n",
            "75%    318194.000000    3898.000000       0.0\n",
            "max    424169.000000    4993.000000       0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x1YpH5ygv62I",
        "colab_type": "text"
      },
      "source": [
        "## 问题\n",
        "### 1、为什么预测prob结果全为0，是代码有问题吗？还是标签不平衡造成的？如何解决？\n",
        "### 2、数据量过大，运行中内存不够，有哪些解决方案？"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ualNlGhg0sdW",
        "colab_type": "code",
        "outputId": "296fda26-e5d4-485e-bf44-77f8ba9c11c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 703
        }
      },
      "source": [
        "#test DIN\n",
        "import numpy as np\n",
        "\n",
        "from deepctr.models import DIN\n",
        "from deepctr.inputs import SparseFeat,VarLenSparseFeat,DenseFeat,get_feature_names\n",
        "\n",
        "def get_xy_fd():\n",
        "\n",
        "    feature_columns = [SparseFeat('user',3,embedding_dim=10),SparseFeat(\n",
        "        'gender', 2,embedding_dim=4), SparseFeat('item_id', 3 + 1,embedding_dim=8), SparseFeat('cate_id', 2 + 1,embedding_dim=4),DenseFeat('pay_score', 1)]\n",
        "    feature_columns += [VarLenSparseFeat(SparseFeat('hist_item_id',vocabulary_size=3 + 1,embedding_dim=8), maxlen=4),\n",
        "                        VarLenSparseFeat(SparseFeat('hist_cate_id',vocabulary_size=2 + 1,embedding_dim=4), maxlen=4)] # ,weight_name='hist_item_id',weight_name='hist_cate_id'\n",
        "\n",
        "    behavior_feature_list = [\"item_id\", \"cate_id\"]\n",
        "    uid = np.array([0, 1, 2])\n",
        "    ugender = np.array([0, 1, 0])\n",
        "    iid = np.array([1, 2, 3])  # 0 is mask value\n",
        "    cate_id = np.array([1, 2, 2])  # 0 is mask value\n",
        "    pay_score = np.array([0.1, 0.2, 0.3])\n",
        "\n",
        "    hist_iid = np.array([[1, 2, 3, 0], [3, 2, 1, 0], [1, 2, 0, 0]])\n",
        "    hist_cate_id = np.array([[1, 2, 2, 0], [2, 2, 1, 0], [1, 2, 0, 0]])\n",
        "\n",
        "    feature_dict = {'user': uid, 'gender': ugender, 'item_id': iid, 'cate_id': cate_id,\n",
        "                    'hist_item_id': hist_iid, 'hist_cate_id': hist_cate_id, 'pay_score': pay_score}\n",
        "    x = {name:feature_dict[name] for name in get_feature_names(feature_columns)}\n",
        "    y = [1, 0, 1]\n",
        "    return x, y, feature_columns, behavior_feature_list\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    x, y, feature_columns, behavior_feature_list = get_xy_fd()\n",
        "    print(x, y, feature_columns, behavior_feature_list)\n",
        "    model = DIN(feature_columns, behavior_feature_list) #这里要求inputs shapes删除维度（/通道数）以后形状相同;通道数位置为-1，即倒数第一个数 \n",
        "    model.compile('adam', 'binary_crossentropy',\n",
        "                  metrics=['binary_crossentropy'])\n",
        "    history = model.fit(x, y, verbose=1, epochs=10, validation_split=0.5)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'user': array([0, 1, 2]), 'gender': array([0, 1, 0]), 'item_id': array([1, 2, 3]), 'cate_id': array([1, 2, 2]), 'pay_score': array([0.1, 0.2, 0.3]), 'hist_item_id': array([[1, 2, 3, 0],\n",
            "       [3, 2, 1, 0],\n",
            "       [1, 2, 0, 0]]), 'hist_cate_id': array([[1, 2, 2, 0],\n",
            "       [2, 2, 1, 0],\n",
            "       [1, 2, 0, 0]])} [1, 0, 1] [SparseFeat(name='user', vocabulary_size=3, embedding_dim=10, use_hash=False, dtype='int32', embedding_name='user', group_name='default_group'), SparseFeat(name='gender', vocabulary_size=2, embedding_dim=4, use_hash=False, dtype='int32', embedding_name='gender', group_name='default_group'), SparseFeat(name='item_id', vocabulary_size=4, embedding_dim=8, use_hash=False, dtype='int32', embedding_name='item_id', group_name='default_group'), SparseFeat(name='cate_id', vocabulary_size=3, embedding_dim=4, use_hash=False, dtype='int32', embedding_name='cate_id', group_name='default_group'), DenseFeat(name='pay_score', dimension=1, dtype='float32'), VarLenSparseFeat(sparsefeat=SparseFeat(name='hist_item_id', vocabulary_size=4, embedding_dim=8, use_hash=False, dtype='int32', embedding_name='hist_item_id', group_name='default_group'), maxlen=4, combiner='mean', length_name=None, weight_name=None, weight_norm=True), VarLenSparseFeat(sparsefeat=SparseFeat(name='hist_cate_id', vocabulary_size=3, embedding_dim=4, use_hash=False, dtype='int32', embedding_name='hist_cate_id', group_name='default_group'), maxlen=4, combiner='mean', length_name=None, weight_name=None, weight_norm=True)] ['item_id', 'cate_id']\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/keras/initializers.py:143: calling RandomNormal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "WARNING:tensorflow:Entity <bound method NoMask.call of <deepctr.layers.utils.NoMask object at 0x7fd408fcb748>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: No module named 'tensorflow_estimator.contrib'\n",
            "WARNING: Entity <bound method NoMask.call of <deepctr.layers.utils.NoMask object at 0x7fd408fcb748>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: No module named 'tensorflow_estimator.contrib'\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/deepctr/layers/sequence.py:270: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "Train on 1 samples, validate on 2 samples\n",
            "Epoch 1/10\n",
            "1/1 [==============================] - 2s 2s/sample - loss: 0.6973 - binary_crossentropy: 0.6973 - val_loss: 0.6939 - val_binary_crossentropy: 0.6939\n",
            "Epoch 2/10\n",
            "1/1 [==============================] - 0s 22ms/sample - loss: 0.6879 - binary_crossentropy: 0.6879 - val_loss: 0.6928 - val_binary_crossentropy: 0.6928\n",
            "Epoch 3/10\n",
            "1/1 [==============================] - 0s 21ms/sample - loss: 0.6786 - binary_crossentropy: 0.6786 - val_loss: 0.6918 - val_binary_crossentropy: 0.6918\n",
            "Epoch 4/10\n",
            "1/1 [==============================] - 0s 19ms/sample - loss: 0.6698 - binary_crossentropy: 0.6698 - val_loss: 0.6910 - val_binary_crossentropy: 0.6910\n",
            "Epoch 5/10\n",
            "1/1 [==============================] - 0s 21ms/sample - loss: 0.6613 - binary_crossentropy: 0.6613 - val_loss: 0.6903 - val_binary_crossentropy: 0.6903\n",
            "Epoch 6/10\n",
            "1/1 [==============================] - 0s 20ms/sample - loss: 0.6527 - binary_crossentropy: 0.6527 - val_loss: 0.6898 - val_binary_crossentropy: 0.6898\n",
            "Epoch 7/10\n",
            "1/1 [==============================] - 0s 20ms/sample - loss: 0.6440 - binary_crossentropy: 0.6440 - val_loss: 0.6892 - val_binary_crossentropy: 0.6892\n",
            "Epoch 8/10\n",
            "1/1 [==============================] - 0s 20ms/sample - loss: 0.6349 - binary_crossentropy: 0.6349 - val_loss: 0.6888 - val_binary_crossentropy: 0.6888\n",
            "Epoch 9/10\n",
            "1/1 [==============================] - 0s 21ms/sample - loss: 0.6255 - binary_crossentropy: 0.6255 - val_loss: 0.6886 - val_binary_crossentropy: 0.6886\n",
            "Epoch 10/10\n",
            "1/1 [==============================] - 0s 21ms/sample - loss: 0.6158 - binary_crossentropy: 0.6158 - val_loss: 0.6888 - val_binary_crossentropy: 0.6888\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}