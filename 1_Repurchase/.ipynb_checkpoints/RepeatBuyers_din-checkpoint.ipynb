{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qc66bhQ3w4gB"
   },
   "source": [
    "# 天猫用户复购预测-DIN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yrVLObec1Tr2"
   },
   "source": [
    "* Content：天猫用户复购预测,使用Attention机制的DNN模型完成预测;\n",
    "* Author:  HuiHui\n",
    "* Date:    2020-03-28\n",
    "* Reference:\n",
    "* 数据集：该数据集包含“双11”前6个月和“双11”当天匿名用户的购物日志，以及显示他们是否为重复购买者的标签信息。\n",
    "    * label: 1'表示'user_id'是'merchant_id'的重复买家，而'0'则相反。'-1'表示'user_id'不是给定商家的新客户\n",
    "    * activity_log: {用户id，商家id}之间的一组交互记录，其中每个记录都是一个动作，表示为“项目id:category id:brand id:time\\u stamp:action\\u type”#'用于分隔两个相邻元素。记录不按任何特定顺序排序"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "iLPjG3VFu92f",
    "outputId": "b6fb2bdb-bf11-4264-ebcf-642580d6dbf8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive') #挂载网盘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "buQv_6iYvKzc"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"/content/gdrive/My Drive/RS/Repeat Buyers Prediction\") #改变当前工作目录到指定的路径"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "id": "06hB_RrTSExt",
    "outputId": "577508fc-e059-48c0-89cd-dfe64da314a4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (4.38.0)\n",
      "TensorFlow 1.x selected.\n"
     ]
    }
   ],
   "source": [
    "!pip install tqdm\n",
    "%tensorflow_version 1.x\n",
    "!pip install -q deepctr[gpu]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "c6o_ntMWXYGS"
   },
   "outputs": [],
   "source": [
    "#!pip uninstall -y tensorflow\n",
    "#!pip install tensorflow-gpu==1.14.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 107
    },
    "colab_type": "code",
    "id": "xcIfzzTkpq4M",
    "outputId": "525e2ba1-a70e-4866-e547-6c00b6a43593"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.6.9 (default, Nov  7 2019, 10:44:02) \n",
      "[GCC 8.3.0]\n",
      "0.7.4\n",
      "1.15.2\n",
      "Default GPU Device: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "#环境\n",
    "import sys\n",
    "print(sys.version)\n",
    "\n",
    "import deepctr\n",
    "print(deepctr.__version__)\n",
    "\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "\n",
    "if tf.test.gpu_device_name():\n",
    "  print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))\n",
    "else:\n",
    "  print(\"Please install GPU version of TF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 125
    },
    "colab_type": "code",
    "id": "wgluqI-xSiKC",
    "outputId": "85f6033d-caf8-4b0d-a189-5dbe2202769d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4995\n",
      "424169\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:108: FutureWarning: Indexing with multiple keys (implicitly converted to a tuple of keys) will be deprecated, use a list instead.\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:121: FutureWarning: Indexing with multiple keys (implicitly converted to a tuple of keys) will be deprecated, use a list instead.\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:149: FutureWarning: Indexing with multiple keys (implicitly converted to a tuple of keys) will be deprecated, use a list instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# time1=time.time()\n",
    "# time2=time.time()\n",
    "# print(time2-time1)\n",
    "\n",
    "# 用户行为，使用format1进行加载\n",
    "# 加载全量样本\n",
    "user_log = pd.read_csv('./Repeat Buyers Prediction DataSet/data_format1/user_log_format1.csv', dtype={'time_stamp':'str'})\n",
    "user_info = pd.read_csv('./Repeat Buyers Prediction DataSet/data_format1/user_info_format1.csv')\n",
    "train_data1 = pd.read_csv('./Repeat Buyers Prediction DataSet/data_format1/train_format1.csv')\n",
    "submission = pd.read_csv('./Repeat Buyers Prediction DataSet/data_format1/test_format1.csv')\n",
    "\n",
    "# # 加载小样本\n",
    "# user_log = pd.read_csv('./Repeat Buyers Prediction DataSet/data_format1_small/sample_user_log.csv', dtype={'time_stamp':'str'})\n",
    "# user_info = pd.read_csv('./Repeat Buyers Prediction DataSet/data_format1_small/sample_user_info.csv')\n",
    "# train_data1 = pd.read_csv('./Repeat Buyers Prediction DataSet/data_format1_small/train.csv')\n",
    "# submission = pd.read_csv('./Repeat Buyers Prediction DataSet/data_format1_small/test.csv')\n",
    "\n",
    "train_data = pd.read_csv('./Repeat Buyers Prediction DataSet/data_format2/train_format2.csv')\n",
    "\n",
    "###### 数据处理 #######\n",
    "train_data1['origin'] = 'train'\n",
    "submission['origin'] = 'test'\n",
    "matrix = pd.concat([train_data1, submission], ignore_index=True, sort=False)\n",
    "#print(matrix.head())\n",
    "\n",
    "# 使用merchant_id（原列名seller_id）\n",
    "user_log.rename(columns={'seller_id':'merchant_id'}, inplace=True)\n",
    "# 格式化\n",
    "user_log['user_id'] = user_log['user_id'].astype('int32')\n",
    "user_log['merchant_id'] = user_log['merchant_id'].astype('int32')\n",
    "user_log['item_id'] = user_log['item_id'].astype('int32')\n",
    "user_log['cat_id'] = user_log['cat_id'].astype('int32')\n",
    "user_log['brand_id'].fillna(0, inplace=True)\n",
    "user_log['brand_id'] = user_log['brand_id'].astype('int32')\n",
    "user_log['time_stamp'] = pd.to_datetime(user_log['time_stamp'], format='%H%M')\n",
    "\n",
    "# 对离散特征做LabelEncoder（0～n-1）\n",
    "lbe_merchant_id=LabelEncoder()\n",
    "lbe_merchant_id.fit(np.r_[0,user_log['merchant_id'].values])\n",
    "user_log['merchant_id']=lbe_merchant_id.transform(user_log['merchant_id'])\n",
    "matrix['merchant_id']=lbe_merchant_id.transform(matrix['merchant_id'])\n",
    "\n",
    "lbe_user_id=LabelEncoder()\n",
    "user_log['user_id']=lbe_user_id.fit_transform(user_log['user_id'])\n",
    "user_info['user_id']=lbe_user_id.transform(user_info['user_id'])\n",
    "matrix['user_id']=lbe_user_id.transform(matrix['user_id'])\n",
    "\n",
    "lbe_item_id=LabelEncoder()\n",
    "user_log['item_id']=lbe_item_id.fit_transform(user_log['item_id'])\n",
    "lbe_cat_id=LabelEncoder()\n",
    "user_log['cat_id']=lbe_cat_id.fit_transform(user_log['cat_id'])\n",
    "lbe_brand_id=LabelEncoder()\n",
    "user_log['brand_id']=lbe_brand_id.fit_transform(user_log['brand_id'])\n",
    "\n",
    "user_log['merchant_id'].max(),user_log['user_id'].max() #统计不同商家个数：4994+1，不同买家的个数：19111+1(小样本时)\n",
    "print(user_log['merchant_id'].max())\n",
    "print(user_log['user_id'].max())\n",
    "matrix = matrix.merge(user_info, on='user_id', how='left')\n",
    "\n",
    "# 1 for <18; 2 for [18,24]; 3 for [25,29]; 4 for [30,34]; 5 for [35,39]; 6 for [40,49]; 7 and 8 for >= 50; 0 and NULL for unknown\n",
    "matrix['age_range'].fillna(0, inplace=True)\n",
    "# 0:female, 1:male, 2:unknown\n",
    "matrix['gender'].fillna(2, inplace=True)\n",
    "matrix['age_range'] = matrix['age_range'].astype('int8')\n",
    "matrix['gender'] = matrix['gender'].astype('int8')\n",
    "matrix['label'] = matrix['label'].astype('str')\n",
    "matrix['user_id'] = matrix['user_id'].astype('int32')\n",
    "matrix['merchant_id'] = matrix['merchant_id'].astype('int32')\n",
    "del user_info, train_data1\n",
    "gc.collect()\n",
    "\n",
    "# User特征处理\n",
    "groups = user_log.groupby(['user_id'])\n",
    "# 用户交互行为数量 u1\n",
    "temp = groups.size().reset_index().rename(columns={0:'u1'})\n",
    "matrix = matrix.merge(temp, on='user_id', how='left')\n",
    "# 使用agg 基于列的聚合操作，统计唯一值的个数 item_id, cat_id, merchant_id, brand_id\n",
    "#temp = groups['item_id', 'cat_id', 'merchant_id', 'brand_id'].nunique().reset_index().rename(columns={'item_id':'u2', 'cat_id':'u3', 'merchant_id':'u4', 'brand_id':'u5'})\n",
    "temp = groups['item_id'].agg([('u2', 'nunique')]).reset_index()\n",
    "matrix = matrix.merge(temp, on='user_id', how='left')\n",
    "temp = groups['cat_id'].agg([('u3', 'nunique')]).reset_index()\n",
    "matrix = matrix.merge(temp, on='user_id', how='left')\n",
    "temp = groups['merchant_id'].agg([('u4', 'nunique')]).reset_index()\n",
    "matrix = matrix.merge(temp, on='user_id', how='left')\n",
    "temp = groups['brand_id'].agg([('u5', 'nunique')]).reset_index()\n",
    "matrix = matrix.merge(temp, on='user_id', how='left')\n",
    "\n",
    "# 时间间隔特征 u6 按照小时\n",
    "temp = groups['time_stamp'].agg([('F_time', 'min'), ('L_time', 'max')]).reset_index()\n",
    "temp['u6'] = (temp['L_time'] - temp['F_time']).dt.seconds/3600 #用户一开始在淘宝买东西和最近一次在淘宝买东西的间隔时间\n",
    "matrix = matrix.merge(temp[['user_id', 'u6']], on='user_id', how='left')\n",
    "# 统计action_type为0，1，2，3的个数（原始操作，没有补0）\n",
    "temp = groups['action_type'].value_counts().unstack().reset_index().rename(columns={0:'u7', 1:'u8', 2:'u9', 3:'u10'})\n",
    "matrix = matrix.merge(temp, on='user_id', how='left')\n",
    "#print(matrix)\n",
    "\n",
    "# 商家特征处理\n",
    "groups = user_log.groupby(['merchant_id'])\n",
    "# 商家被交互行为数量 m1\n",
    "temp = groups.size().reset_index().rename(columns={0:'m1'})\n",
    "matrix = matrix.merge(temp, on='merchant_id', how='left')\n",
    "# 统计商家被交互的user_id, item_id, cat_id, brand_id 唯一值\n",
    "temp = groups['user_id', 'item_id', 'cat_id', 'brand_id'].nunique().reset_index().rename(columns={'user_id':'m2', 'item_id':'m3', 'cat_id':'m4', 'brand_id':'m5'})\n",
    "matrix = matrix.merge(temp, on='merchant_id', how='left')\n",
    "# 统计商家被交互的action_type 唯一值\n",
    "temp = groups['action_type'].value_counts().unstack().reset_index().rename(columns={0:'m6', 1:'m7', 2:'m8', 3:'m9'})\n",
    "matrix = matrix.merge(temp, on='merchant_id', how='left')\n",
    "# 按照merchant_id 统计随机负采样的个数\n",
    "temp = train_data[train_data['label']==-1].groupby(['merchant_id']).size().reset_index().rename(columns={0:'m10'})\n",
    "matrix = matrix.merge(temp, on='merchant_id', how='left')\n",
    "\n",
    "# 按照user_id, merchant_id分组\n",
    "groups = user_log.groupby(['user_id', 'merchant_id'])\n",
    "temp = groups.size().reset_index().rename(columns={0:'um1'}) #统计行为个数\n",
    "matrix = matrix.merge(temp, on=['user_id', 'merchant_id'], how='left')\n",
    "temp = groups['item_id', 'cat_id', 'brand_id'].nunique().reset_index().rename(columns={'item_id':'um2', 'cat_id':'um3', 'brand_id':'um4'}) #统计item_id, cat_id, brand_id唯一个数\n",
    "matrix = matrix.merge(temp, on=['user_id', 'merchant_id'], how='left')\n",
    "temp = groups['action_type'].value_counts().unstack().reset_index().rename(columns={0:'um5', 1:'um6', 2:'um7', 3:'um8'})#统计不同action_type唯一个数\n",
    "matrix = matrix.merge(temp, on=['user_id', 'merchant_id'], how='left')\n",
    "temp = groups['time_stamp'].agg([('first', 'min'), ('last', 'max')]).reset_index()\n",
    "temp['um9'] = (temp['last'] - temp['first']).dt.seconds/3600\n",
    "temp.drop(['first', 'last'], axis=1, inplace=True)\n",
    "matrix = matrix.merge(temp, on=['user_id', 'merchant_id'], how='left') #统计时间间隔\n",
    "\n",
    "#用户购买点击比\n",
    "matrix['r1'] = matrix['u9']/matrix['u7'] \n",
    "#商家购买点击比\n",
    "matrix['r2'] = matrix['m8']/matrix['m6'] \n",
    "#不同用户不同商家购买点击比\n",
    "matrix['r3'] = matrix['um7']/matrix['um5']\n",
    "matrix.fillna(0, inplace=True)\n",
    "# # 修改age_range字段名称为 age_0, age_1, age_2... age_8\n",
    "temp = pd.get_dummies(matrix['age_range'], prefix='age')\n",
    "matrix = pd.concat([matrix, temp], axis=1)\n",
    "temp = pd.get_dummies(matrix['gender'], prefix='g')\n",
    "matrix = pd.concat([matrix, temp], axis=1)\n",
    "matrix.drop(['age_range', 'gender'], axis=1, inplace=True)\n",
    "\n",
    "lbe_action_type={0:1,1:2,2:3,3:4}\n",
    "user_log['action_type']=user_log['action_type'].map(lbe_action_type) #action_type映射成1,2,3,4；方便后面用0补缺固定长度\n",
    "\n",
    "# 用户行为sequence\n",
    "# 把user_log里同user的这些数据合并成一个list\n",
    "temp=pd.DataFrame(user_log.groupby('user_id')['merchant_id','action_type'].agg(lambda x:list(x)))\n",
    "# 列名称改成hist_merchant_id 和 hist_action_type \n",
    "temp.columns=['hist_merchant_id','hist_action_type']\n",
    "matrix = matrix.merge(temp, on=['user_id'], how='left')\n",
    "\n",
    "# 截取，补缺到定长M个\n",
    "M=500\n",
    "for feature in ['hist_merchant_id','hist_action_type']:\n",
    "    matrix[feature]=matrix[feature].map(lambda x:np.array(x+[0]*(M-len(x)))[:M])\n",
    "#print(matrix.info())\n",
    "\n",
    "# 分割训练数据和测试数据\n",
    "train_data = matrix[matrix['origin'] == 'train'].drop(['origin'], axis=1)\n",
    "test_data = matrix[matrix['origin'] == 'test'].drop(['label', 'origin'], axis=1)\n",
    "train_X, train_y = train_data.drop(['label'], axis=1), train_data['label']\n",
    "\n",
    "#保存特征文件，调试不同算法模型\n",
    "train_X.to_csv('./train_X.csv', index=False)\n",
    "train_y.to_csv('./train_y.csv', index=False)\n",
    "test_data.to_csv('./test_data.csv', index=False)\n",
    "print(\"ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 757
    },
    "colab_type": "code",
    "id": "zDWFbcYOALzO",
    "outputId": "62f70c90-f909-4315-c678-db74bce81ad2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   user_id  merchant_id  prob origin\n",
      "0   163968         4605   NaN   test\n",
      "1   360576         1581   NaN   test\n",
      "2    98688         1964   NaN   test\n",
      "3    98688         3645   NaN   test\n",
      "4   295296         3361   NaN   test\n",
      "ok\n",
      "M= 500\n",
      "[SparseFeat(name='user_id', vocabulary_size=424170, embedding_dim=10, use_hash=False, dtype='int32', embedding_name='user_id', group_name='default_group'), SparseFeat(name='merchant_id', vocabulary_size=4996, embedding_dim=8, use_hash=False, dtype='int32', embedding_name='merchant_id', group_name='default_group'), DenseFeat(name='prob', dimension=1, dtype='float32'), DenseFeat(name='u1', dimension=1, dtype='float32'), DenseFeat(name='u2', dimension=1, dtype='float32'), DenseFeat(name='u3', dimension=1, dtype='float32'), DenseFeat(name='u4', dimension=1, dtype='float32'), DenseFeat(name='u5', dimension=1, dtype='float32'), DenseFeat(name='u6', dimension=1, dtype='float32'), DenseFeat(name='u7', dimension=1, dtype='float32'), DenseFeat(name='u8', dimension=1, dtype='float32'), DenseFeat(name='u9', dimension=1, dtype='float32'), DenseFeat(name='u10', dimension=1, dtype='float32'), DenseFeat(name='m1', dimension=1, dtype='float32'), DenseFeat(name='m2', dimension=1, dtype='float32'), DenseFeat(name='m3', dimension=1, dtype='float32'), DenseFeat(name='m4', dimension=1, dtype='float32'), DenseFeat(name='m5', dimension=1, dtype='float32'), DenseFeat(name='m6', dimension=1, dtype='float32'), DenseFeat(name='m7', dimension=1, dtype='float32'), DenseFeat(name='m8', dimension=1, dtype='float32'), DenseFeat(name='m9', dimension=1, dtype='float32'), DenseFeat(name='m10', dimension=1, dtype='float32'), DenseFeat(name='um1', dimension=1, dtype='float32'), DenseFeat(name='um2', dimension=1, dtype='float32'), DenseFeat(name='um3', dimension=1, dtype='float32'), DenseFeat(name='um4', dimension=1, dtype='float32'), DenseFeat(name='um5', dimension=1, dtype='float32'), DenseFeat(name='um6', dimension=1, dtype='float32'), DenseFeat(name='um7', dimension=1, dtype='float32'), DenseFeat(name='um8', dimension=1, dtype='float32'), DenseFeat(name='um9', dimension=1, dtype='float32'), DenseFeat(name='r1', dimension=1, dtype='float32'), DenseFeat(name='r2', dimension=1, dtype='float32'), DenseFeat(name='r3', dimension=1, dtype='float32'), DenseFeat(name='age_0', dimension=1, dtype='float32'), DenseFeat(name='age_1', dimension=1, dtype='float32'), DenseFeat(name='age_2', dimension=1, dtype='float32'), DenseFeat(name='age_3', dimension=1, dtype='float32'), DenseFeat(name='age_4', dimension=1, dtype='float32'), DenseFeat(name='age_5', dimension=1, dtype='float32'), DenseFeat(name='age_6', dimension=1, dtype='float32'), DenseFeat(name='age_7', dimension=1, dtype='float32'), DenseFeat(name='age_8', dimension=1, dtype='float32'), DenseFeat(name='g_0', dimension=1, dtype='float32'), DenseFeat(name='g_1', dimension=1, dtype='float32'), DenseFeat(name='g_2', dimension=1, dtype='float32'), SparseFeat(name='action_type', vocabulary_size=5, embedding_dim=4, use_hash=False, dtype='int32', embedding_name='action_type', group_name='default_group'), VarLenSparseFeat(sparsefeat=SparseFeat(name='hist_merchant_id', vocabulary_size=424170, embedding_dim=8, use_hash=False, dtype='int32', embedding_name='hist_merchant_id', group_name='default_group'), maxlen=500, combiner='mean', length_name=None, weight_name=None, weight_norm=True), VarLenSparseFeat(sparsefeat=SparseFeat(name='hist_action_type', vocabulary_size=5, embedding_dim=4, use_hash=False, dtype='int32', embedding_name='hist_action_type', group_name='default_group'), maxlen=500, combiner='mean', length_name=None, weight_name=None, weight_norm=True)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 260864/260864 [00:00<00:00, 2331063.48it/s]\n",
      "100%|██████████| 260864/260864 [00:00<00:00, 2328345.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 208691 samples, validate on 52173 samples\n",
      "Epoch 1/10\n",
      "208691/208691 [==============================] - 23s 108us/sample - loss: 0.9433 - binary_crossentropy: 0.9433 - val_loss: 0.9431 - val_binary_crossentropy: 0.9431\n",
      "Epoch 2/10\n",
      "208691/208691 [==============================] - 22s 104us/sample - loss: 0.9433 - binary_crossentropy: 0.9433 - val_loss: 0.9431 - val_binary_crossentropy: 0.9431\n",
      "Epoch 3/10\n",
      "208691/208691 [==============================] - 22s 106us/sample - loss: 0.9433 - binary_crossentropy: 0.9433 - val_loss: 0.9431 - val_binary_crossentropy: 0.9431\n",
      "Epoch 4/10\n",
      "208691/208691 [==============================] - 22s 105us/sample - loss: 0.9433 - binary_crossentropy: 0.9433 - val_loss: 0.9431 - val_binary_crossentropy: 0.9431\n",
      "Epoch 5/10\n",
      "208691/208691 [==============================] - 22s 104us/sample - loss: 0.9433 - binary_crossentropy: 0.9433 - val_loss: 0.9431 - val_binary_crossentropy: 0.9431\n",
      "Epoch 6/10\n",
      "208691/208691 [==============================] - 22s 104us/sample - loss: 0.9433 - binary_crossentropy: 0.9433 - val_loss: 0.9431 - val_binary_crossentropy: 0.9431\n",
      "Epoch 7/10\n",
      "208691/208691 [==============================] - 22s 104us/sample - loss: 0.9433 - binary_crossentropy: 0.9433 - val_loss: 0.9431 - val_binary_crossentropy: 0.9431\n",
      "Epoch 8/10\n",
      "208691/208691 [==============================] - 22s 104us/sample - loss: 0.9433 - binary_crossentropy: 0.9433 - val_loss: 0.9431 - val_binary_crossentropy: 0.9431\n",
      "Epoch 9/10\n",
      "208691/208691 [==============================] - 22s 103us/sample - loss: 0.9433 - binary_crossentropy: 0.9433 - val_loss: 0.9431 - val_binary_crossentropy: 0.9431\n",
      "Epoch 10/10\n",
      "208691/208691 [==============================] - 22s 103us/sample - loss: 0.9433 - binary_crossentropy: 0.9433 - val_loss: 0.9431 - val_binary_crossentropy: 0.9431\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 261477/261477 [00:00<00:00, 2427937.70it/s]\n",
      "100%|██████████| 261477/261477 [00:00<00:00, 2496333.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   user_id  merchant_id  prob origin\n",
      "0   163968         4605   0.0   test\n",
      "1   360576         1581   0.0   test\n",
      "2    98688         1964   0.0   test\n",
      "3    98688         3645   0.0   test\n",
      "4   295296         3361   0.0   test\n"
     ]
    }
   ],
   "source": [
    "##### 使用DIN模型 ######\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import log_loss\n",
    "from deepctr.inputs import SparseFeat,VarLenSparseFeat,DenseFeat,get_feature_names\n",
    "from deepctr.models import DIN, DIEN, DSIN\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "M=500\n",
    "#读取submission,存储结果\n",
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "submission = pd.read_csv('./Repeat Buyers Prediction DataSet/data_format1/test_format1.csv') # 加载全量样本时\n",
    "# submission = pd.read_csv('./Repeat Buyers Prediction DataSet/data_format1_small/test.csv') # 加载小样本时\n",
    "submission['origin'] = 'test'\n",
    "print(submission.head())\n",
    "\n",
    "#读取保存好的特征文件，注意这里重新读取会将'hist_merchant_id','hist_action_type'读取成str,而不是list，因此需要将train_X和test_data中的hist_merchant_id，hist_action_type转回list\n",
    "train_X=pd.read_csv('./train_X.csv')\n",
    "train_y=pd.read_csv('./train_y.csv')\n",
    "test_data=pd.read_csv('./test_data.csv')\n",
    "#将train_X和test_data中的hist_merchant_id，hist_action_type转回list\n",
    "def strlist_to_list(s):\n",
    "  s=re.sub('[\\[\\]]','',s)#删除中括号\n",
    "  s=s.replace(\"\\n\", \"\")#删除换行符号\n",
    "  s=s.split() # 以空格（一个或多个）分隔\n",
    "  s=[float(num) for num in s]\n",
    "  return s\n",
    "train_X['hist_merchant_id']=train_X['hist_merchant_id'].map(strlist_to_list)\n",
    "train_X['hist_action_type']=train_X['hist_action_type'].map(strlist_to_list)\n",
    "test_data['hist_merchant_id']=test_data['hist_merchant_id'].map(strlist_to_list)\n",
    "test_data['hist_action_type']=test_data['hist_action_type'].map(strlist_to_list)\n",
    "#print(type(train_X['hist_merchant_id'].values))\n",
    "print(\"ok\")\n",
    "\n",
    "train_X['action_type']=3 #因为这里用户在商家是购买过商品的，所以添加了一列action_type，且设为3\n",
    "\n",
    "#格式转换（将之前的特征名封装成了一个类）\n",
    "feature_columns = []\n",
    "for column in train_X.columns:\n",
    "  if column != 'hist_merchant_id' and column != 'hist_action_type':\n",
    "    #print(column)\n",
    "    num = train_X[column].nunique()\n",
    "    if num > 10000:\n",
    "        dim = 10\n",
    "    else:\n",
    "        if num > 1000:\n",
    "            dim = 8\n",
    "        else:\n",
    "            dim = 4\n",
    "    #print(num)\n",
    "    if column  == 'user_id':\n",
    "        feature_columns += [SparseFeat(column, 424169+1, embedding_dim=dim)]\n",
    "    elif column  == 'merchant_id':\n",
    "        feature_columns += [SparseFeat(column, 4995+1, embedding_dim=dim)]\n",
    "    elif column  == 'action_type':\n",
    "        feature_columns += [SparseFeat(column, 4+1, embedding_dim=dim)]\n",
    "    else:\n",
    "        feature_columns += [DenseFeat(column, 1)]\n",
    "\n",
    "#print(train_X['hist_merchant_id'].shape)\n",
    "#M = len(train_X['hist_merchant_id'])\n",
    "print('M=', M)\n",
    "\n",
    "# maxlen为历史信息的长度，vocabulary_size为onehot的长度；VarLenSparseFeat为序列类型特征\n",
    "# 注意正确使用VarLenSparseFeat，不要加参数：weight_name='hist_merchant_id'，weight_name='hist_action_type'\n",
    "feature_columns += [VarLenSparseFeat(SparseFeat('hist_merchant_id', vocabulary_size=424169+1, embedding_dim=8), maxlen=M),\n",
    "                   VarLenSparseFeat(SparseFeat('hist_action_type', vocabulary_size=4+1, embedding_dim=4),maxlen=M)]\n",
    "hist_features=['merchant_id','action_type']\n",
    "print(feature_columns)\n",
    "\n",
    "# 使用DIN模型\n",
    "# 这里要求inputs shapes删除维度（/通道数）以后形状相同;通道数位置为-1，即倒数第一个数\n",
    "model=DIN(feature_columns, hist_features)\n",
    "# 使用Adam优化器，二分类的交叉熵\n",
    "model.compile('adam', 'binary_crossentropy', metrics=['binary_crossentropy'])\n",
    "# 组装train_model_input，得到feature names，将train_X转换为字典格式\n",
    "feature_names=list(train_X.columns)\n",
    "train_model_input ={name:train_X[name].values for name in feature_names}\n",
    "\n",
    "# histroy输入必须是二维数组\n",
    "#进度条\n",
    "from tqdm import tqdm\n",
    "for fea in ['hist_merchant_id','hist_action_type']:\n",
    "    l = []\n",
    "    for i in tqdm(train_model_input[fea]):\n",
    "        l.append(i)\n",
    "    train_model_input[fea]=np.array(l) #转换成二维数组\n",
    "\n",
    "history = model.fit(train_model_input, train_y, verbose=True, epochs=10, validation_split=0.2,batch_size=512)\n",
    "\n",
    "# 转换test__model_input\n",
    "test_data['action_type']=3\n",
    "test_model_input = {name:test_data[name].values for name in feature_names}#字典\n",
    "from tqdm import tqdm\n",
    "for fea in ['hist_merchant_id','hist_action_type']:\n",
    "    l = []\n",
    "    for i in tqdm(test_model_input[fea]):\n",
    "        l.append(i)\n",
    "    test_model_input[fea]=np.array(l)#二维数组\n",
    "\n",
    "# 得到预测结果\n",
    "prob = model.predict(test_model_input)\n",
    "submission['prob'] = prob\n",
    "print(submission.head())\n",
    "submission.drop(['origin'], axis=1, inplace=True)\n",
    "submission.to_csv('./prediction.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 179
    },
    "colab_type": "code",
    "hide_input": false,
    "id": "9XT-AcfGtDBl",
    "outputId": "f0b84fb0-8fc6-45ee-fd77-d8a3bce83ed7",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             user_id    merchant_id      prob\n",
      "count  261477.000000  261477.000000  261477.0\n",
      "mean   212121.259128    2539.620077       0.0\n",
      "std    122480.366678    1451.697856       0.0\n",
      "min         2.000000       2.000000       0.0\n",
      "25%    106317.000000    1340.000000       0.0\n",
      "50%    212289.000000    2482.000000       0.0\n",
      "75%    318194.000000    3898.000000       0.0\n",
      "max    424169.000000    4993.000000       0.0\n"
     ]
    }
   ],
   "source": [
    "print(submission.describe())"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "x1YpH5ygv62I"
   ],
   "machine_shape": "hm",
   "name": "RepeatBuyers_din.ipynb",
   "provenance": []
  },
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
